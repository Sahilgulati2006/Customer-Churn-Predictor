# -*- coding: utf-8 -*-
"""Customer Churn Predictor - 1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cOuFHIDIokKKx6hOfN7RAg9EL0t9FgfE

# **Import Necessary Libraries**
"""

import os
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dropout

"""# **Setup Kaggle API to Download Dataset**"""

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

!kaggle competitions download -c customer-churn-prediction-2020

"""# **Extract the Dataset**"""

from zipfile import ZipFile
file_name = '/content/customer-churn-prediction-2020.zip'
with ZipFile(file_name,'r') as zip:
  zip.extractall()
  print('file extracted')

!ls

"""# **Load Train and Test Data**"""

train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

train_df.head()

train_df.info()

train_df.isnull().sum()

"""# **Encode Categorical Variables**"""

train_df = pd.get_dummies(train_df, drop_first=True)
test_df = pd.get_dummies(test_df, drop_first=True)

"""# **Align Test Data with Training Columns**"""

test_df = test_df.reindex(columns=train_df.columns, fill_value=0)

test_df.head()

"""
# **Split Features and Target Variable**"""

from sklearn.model_selection import train_test_split

X = train_df.drop(['churn_yes'], axis=1)
Y = train_df['churn_yes']

X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 3)

print(X_train.shape, X_test.shape, Y_test.shape, Y_train.shape)

"""# **Standardize Features**"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_std = scaler.fit_transform(X_train)
X_test_std = scaler.transform(X_test)

"""# **Define the Neural Network Model**"""

model = keras.Sequential([
    keras.layers.Flatten(input_shape = (69,)),
    keras.layers.Dense(64, activation = 'relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(32, activation = 'relu'),
    keras.layers.Dropout(0.3),
    keras.layers.Dense(2, activation = 'sigmoid')
])

from tensorflow.keras import regularizers
model.add(keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))

"""# **Compile the Model**"""

model.compile( optimizer = 'Nadam',
              loss = 'sparse_categorical_crossentropy',
               metrics = ['accuracy'])

"""# **Handle Imbalanced Dataset with SMOTE**"""

from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_train_res, Y_train_res = smote.fit_resample(X_train_std, Y_train)

print("Before SMOTE:", Y_train.value_counts())
print("After SMOTE:", pd.Series(Y_train_res).value_counts())

"""# **Train the Model with Class Weights**"""

class_weights = {0: 1, 1: 10}
history = model.fit(X_train_res, Y_train_res, validation_split=0.3, epochs = 75)

"""# **Evaluate the Model on Test Data**"""

loss, accuracy  = model.evaluate(X_test_std, Y_test)
print(loss)
print(accuracy)

"""# **Visualize Training History**"""

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='lower right')
plt.show()


plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')
plt.show()

"""# **Generate Classification Report and Confusion Matrix**"""

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Get predicted probabilities
y_pred_prob = model.predict(X_test_std)[:, 1]  # Probabilities for the "Churn" class

# Set a custom threshold
threshold = 0.4  # Adjust this value as needed

# Convert probabilities to binary predictions based on the threshold
y_pred_custom = (y_pred_prob >= threshold).astype(int)

# Print classification report
print(f"classification report: {classification_report(Y_test, y_pred_custom)}")

# Confusion matrix
cm = confusion_matrix(Y_test, y_pred_custom)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=["No Churn", "Churn"], yticklabels=["No Churn", "Churn"])
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""# **Feature Importance Using Random Forest**"""

from sklearn.ensemble import RandomForestClassifier

# Fit a random forest model
rf = RandomForestClassifier()
rf.fit(X_train_std, Y_train)

# Get feature importances
importances = rf.feature_importances_

# Create a DataFrame with feature names and their importance scores
feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})

# Sort the features by importance
print(feature_importance.sort_values(by='Importance', ascending=False))

"""# **Save the Model and Scaler**"""

import joblib

# Save the model
joblib.dump(model, 'churn_model.pkl')

# Save the scaler
joblib.dump(scaler, 'scaler.pkl')